[
  {
    "objectID": "warmup-exercises.html",
    "href": "warmup-exercises.html",
    "title": "Warmup Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\vz}{\\mathbf{z}}\n\\newcommand{\\norm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\abs}[1]{\\lvert #1 \\rvert}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "warmup-exercises.html#sec-perceptron",
    "href": "warmup-exercises.html#sec-perceptron",
    "title": "Warmup Exercises",
    "section": "Perceptron",
    "text": "Perceptron\n\nPart 1\nSketch the line in \\(\\R^2\\) described by the equation \\[\n\\bracket{\\vw, \\vx}  =  b\\;,\n\\tag{1}\\]\nwhere \\(\\vw = \\paren{1, -\\frac{1}{2}}^T \\in \\R^2\\) and \\(b = \\frac{1}{2}\\). Here, \\(\\bracket{\\vw, \\vx} = \\sum_{i = 1}^n w_i x_i\\) is the inner product (or dot product) between the vectors \\(\\vw\\) and \\(\\vw\\).\n\n\nPart 2\nWrite a quick Python function called perceptron_classify(w, b, x). w and x should both be 1d numpy arrays of the same length, and b should be a scalar. Your function should return 0 if \\(\\bracket{\\vw, \\vx} &lt; b\\) and 1 if \\(\\bracket{\\vw, \\vx} \\geq b\\). An excellent solution will use neither a for-loop nor an if-statement.\nVerify that your function works on a few simple examples.\n\n\nPart 3\nConsider a line of the general form of Equation 1. Let’s allow \\(\\vx\\) and \\(\\vw\\) to both be \\(n\\)-dimensional, so that this equation defines a hyperplane in \\(\\R^n\\). Suppose that we wanted to represent the same hyperplane in \\(\\R^n\\) using an equation of the form\n\\[\n\\bracket{\\tilde{\\vw}, \\tilde{\\vx}} = 0\\;.\n\\tag{2}\\]\nfor some \\(\\tilde{\\vw} \\in \\R^{n+1}\\). Define \\(\\tilde{\\vx} = (\\vx, 1)\\). How could you define \\(\\tilde{\\vw}\\) to make Equation 2 equivalent to Equation 1?"
  },
  {
    "objectID": "warmup-exercises.html#sec-convexity",
    "href": "warmup-exercises.html#sec-convexity",
    "title": "Warmup Exercises",
    "section": "Convexity",
    "text": "Convexity\nAs you learned in Daumé, informally, a convex function is a function that is “bowl-shaped.” Hardt and Recht give the formal definition, which has the benefit of applying to functions of many variables.\n\nPart 1\nConsider the 0-1 step function that I’ve plotted below:\n\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\nfig, ax = plt.subplots(1, 1) \ny_hat = np.linspace(-1, 1, 101)\n\nloss = lambda y_hat, y: 1 - 1*(y_hat*y &gt; 0)\n\nax.set(xlabel = r\"$\\hat{y}$\", \n       ylabel = r\"$\\ell(\\hat{y}, y)$\")\n\nax.plot(y_hat, loss(y_hat, 1))\n\n\n\n\nShow pictorially that this function is not convex. No proof needed – just the right drawing.\n\n\nPart 2: Second Derivative Test\nAnother way to tell whether a function is convex is to check its second derivative. If a function \\(f:S\\rightarrow \\R\\) has a convex domain \\(S\\subseteq \\R\\), if \\(f\\) is everywhere twice-differentiable, and if \\(\\frac{d^2f(z_0)}{dz^2} &gt; 0\\) for all \\(z_0 \\in S\\), then \\(f\\) is convex.\nUse the second derivative test to check that the following two functions are convex: The base of the logarithm doesn’t really matter, but for this course it is always most convenient to assume logs base \\(e\\), which you might also have seen written \\(\\ln\\).\n\\[\n\\begin{aligned}\nf(z) &= - \\log z \\\\\ng(z) &= - \\log(1-z)\\;.\n\\end{aligned}\n\\]\n\n\nPart 3: Plotting Practice\nIn a Jupyter notebook, write a simple program to plot each of the functions \\(f\\) and \\(g\\) from Part 2. Some of the Part 1 code is likely to help you.\n\n\nPart 4: Convexity in Many Variables\nRecall the Hardt and Recht definition of convexity: a function \\(f:\\R^p \\rightarrow \\R\\) is convex if, for any \\(\\lambda \\in [0,1]\\) and any points \\(\\vz_1, \\vz_2 \\in \\R^p\\),\n\\[\nf(\\lambda \\vz_1 + (1-\\lambda)\\vz_2) \\leq \\lambda f(\\vz_1) + (1-\\lambda)f(\\vz_2)\\;.\n\\]\nUsing this definition, write a short mathematical proof that the function \\(f(\\vz) = \\norm{\\vz} = \\sqrt{\\bracket{\\vz, \\vz}}\\) is convex. You will want to use the triangle inequality, which says that \\(\\norm{\\vz_1 + \\vz_2} \\leq \\norm{\\vz_1} + \\norm{\\vz_2}\\). This proof requires just a few lines if you carefully use your definitions!"
  },
  {
    "objectID": "warmup-exercises.html#sec-gradient-descent",
    "href": "warmup-exercises.html#sec-gradient-descent",
    "title": "Warmup Exercises",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nConsider the quadratic function \\(g(z) = \\frac{1}{2}az^2 + bz + c\\).\n\nProve that \\(g\\) has a critical point at the point \\(z^* = -\\frac{b}{a}\\) (hint: solve \\(g'(z^*) = 0\\)).\nWhat must be true about the constants \\(a\\), \\(b\\), and \\(c\\) to ensure that this point is a local minimum of \\(g\\)? (Hint: second derivative test).\nSuppose now that we are able to evaluate the function \\(g\\), as well as its derivative \\(g'\\), but not able to use algebra to find \\(z^*\\) (this mirrors our situation in most practical problems). Instead, we are going to use the following algorithm to attempt to approximate \\(z^*\\):\n\nBegin with some initial guess \\(z^{(0)}\\).\nIn each time-step \\(t\\), compute \\(z^{(t+1)} \\gets z^{(t)} - \\alpha g'(z^{(t)})\\), where \\(\\alpha &gt; 0\\) is the learning rate.\nIn practice we would need to specify a stopping criterion, but for this theoretical problem we don’t need to worry about it.\n\nUsing algebra, prove that for any timestep \\(t\\), \\[\n(z^* - z^{(t+1)})^2 = (a\\alpha - 1)^2(z^* - z^{(t)})^2\\;.\n\\]\nLet’s think of \\(\\abs{z^* - z^{(t)}}\\) as the error in our current estimate \\(z^{(t)}\\). Using the recurrence above, conclude that, for any \\(t\\), the error \\(\\abs{z^* - z^{(t)}}\\) satisfies \\[\n\\abs{z^* - z^{(t)}} = \\abs{a\\alpha - 1}^{t}\\abs{z^* - z^{(0)}}\\;.\n\\]\nFor \\(\\alpha \\in (\\alpha_*, \\alpha^*)\\), we are guaranteed that the error \\(\\abs{z^* - z^{(t)}}\\rightarrow 0\\) as \\(t\\rightarrow \\infty\\). What are \\(\\alpha_*\\) and \\(\\alpha^*\\)?\n\nSuppose that \\(\\alpha\\) is within the necessary range. I want to guarantee that \\(\\abs{z^* - z^{(t)}} &lt; \\epsilon\\) for some small \\(\\epsilon &gt; 0\\) (in practice we often call this the tolerance). Conclude that the number of steps necessary to reach this tolerance is no greater than \\[\n\\bar{t} = \\frac{ \\log \\epsilon - \\log \\abs{z^* - z^{(0)}}}{\\log \\abs{a\\alpha - 1}}\\;.\n\\]\n\nIgnoring constants with respect to \\(\\epsilon\\), we say that this algorithm for finding the minimum of \\(g\\) with tolerance \\(\\epsilon\\) has a \\(\\log \\epsilon\\) a convergence rate."
  },
  {
    "objectID": "warmup-exercises.html#sec-gradient-descent-2",
    "href": "warmup-exercises.html#sec-gradient-descent-2",
    "title": "Warmup Exercises",
    "section": "Gradient Descent (Again)",
    "text": "Gradient Descent (Again)\nConsider the function \\(f(w_0, w_1) = \\sin(w_0w_1)\\). You can define this function like this:\n\nimport numpy as np\ndef f(w):\n    return np.sin(w[0]*w[1])\n\nMathematically, the gradient of this function is\n\\[\\nabla f(w_0, w_1) = (w_1\\cos w_0w_1, w_0 \\cos w_0w_1)^T.\\]\n\nImplement a simple loop that uses gradient descent to find a minimum of this function.\n\nYou’ll have to choose the learning rate \\(\\alpha\\).\nThe np.cos() function will be useful for programming the gradient.\nIt’s not the fastest approach, but if you’re not show how to program the gradient you can always first implement it as a list of two floats, and then use np.array(my_list) to convert it into a numpy array.\nYou’ll also need to pick a random starting guess.\n\nFind two initial guesses for the parameter vector \\(\\vw\\) such that you get two different final minimizers (this is possible because \\(f\\) is not convex)."
  },
  {
    "objectID": "warmup-exercises.html#sec-overfitting",
    "href": "warmup-exercises.html#sec-overfitting",
    "title": "Warmup Exercises",
    "section": "Overfitting and the Scientific Method",
    "text": "Overfitting and the Scientific Method\n Image from Wikipedia.\nIn the scientific method, it is often emphasized that we need to formulate a hypothesis before performing an experiment. It’s fine for the hypothesis to be based on previous experiments. However, the scientific method never allows us to perform an experiment, formulate a hypothesis, and then say that the experiment supported the (new) hypothesis.\nWe can think of scientific theories as systems of thought that help us make predictions about new phenomena. With this in mind, please write a short paragraph explaining the importance of hypothesis-first science using the language of machine learning. In your explanation, please use the following vocabulary:\n\nTraining data.\nTraining accuracy.\nValidation/testing data.\nValidation/testing accuracy.\nOverfitting."
  },
  {
    "objectID": "warmup-exercises.html#sec-erm",
    "href": "warmup-exercises.html#sec-erm",
    "title": "Warmup Exercises",
    "section": "The Coin-Flipping Game",
    "text": "The Coin-Flipping Game\nLet’s play a game! Here is the setup:\nI have a coin with probability of heads equal to \\(p \\in [0,1]\\). I am going to ask you to pick a number \\(\\hat{p} \\in [0,1]\\). Then, I flip my coin.\nThis game is more fun for me than it is for you.\n\nIf my coin comes up heads, you give me \\(-\\log \\hat{p}\\) dollars.\nIf my coin comes up tails, you give me \\(-\\log (1-\\hat{p})\\) dollars.\n\n\nPart 1\nCompute the expected amount of money you will give me when we play this game in terms of \\(p\\) and \\(\\hat{p}\\). Call this quantity \\(R(\\hat{p}, p)\\). This is the risk of the guess \\(\\hat{p}\\).\n\n\nPart 2\nTake the derivative and set it equal to 0! Don’t forget to check that you’ve found a minimum of \\(R(\\hat{p}, p)\\) rather than a maximum or an inflection point.\nSuppose I tell you the value of \\(p\\). Write a mathematical proof to show that your best choice of \\(\\hat{p}\\) (the one that loses you the least money) is \\(\\hat{p} = p\\).\n\n\nPart 3\nNow suppose that I don’t tell you the true value of \\(p\\). Instead, I let you observe \\(n\\) coin flips before asking you to make your guess. Describe:\n\nA suggestion for choosing \\(\\hat{p}\\) based only on the results of the previous flips.\nA way to estimate the risk (expected amount of money lost) based only on the results of the previous flips.\n\nYour answer should depend on \\(\\hat{p}\\) but not on \\(p\\)!"
  },
  {
    "objectID": "warmup-exercises.html#sec-classification-rates-2",
    "href": "warmup-exercises.html#sec-classification-rates-2",
    "title": "Warmup Exercises",
    "section": "Balancing Classification Rates",
    "text": "Balancing Classification Rates\nYou can do this first part just by copying and pasting lecture code. It doesn’t matter much how good your model is – just make sure you’re able to get predictions.\nUse the code from our recent lecture to download the Titanic data set as a Pandas data frame and train a model on the training data. Then download the test data. Compute y_pred, the vector of predictions of your model on the test data.\nThen, write a function that verifies eq. (2.6) in Alexandra Chouldechova’s paper “Fair Prediction with disparate impact.” Here’s what your function should do:\nThe positive predictive value is \\(\\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\).\n\nGiven vectors y_pred of predictions and y_test of actual labels, compute the False Negative Rate (FNR), False Positive Rate (FPR), prevalence \\(p\\), and positive predictive value (PPV).\nReturn as a tuple the lefthand side and righthand side of eq. (2.6) in Chouldechova.\nVerify that the two numbers are equal!"
  },
  {
    "objectID": "warmup-exercises.html#sec-limits-quantitative",
    "href": "warmup-exercises.html#sec-limits-quantitative",
    "title": "Warmup Exercises",
    "section": "Limits of The Quantitative Approach to Discrimination",
    "text": "Limits of The Quantitative Approach to Discrimination\nI’ll give you each a number in Slack. The numbers correspond to the following sections of Narayanan (2022). These are:\n\nThe null hypothesis allocates the burden of proof (p. 7-8)\nCompounding inequality is far below the radar of quantitative methods (p. 9-10)\nSnapshot datasets hide discrimination (p. 10-11)\nExplaining away discrimination (p. 12-13)\nWhat counts as evidence is a subjective choice (p. 5-7)\n\nFor your assigned section, please write a short paragraph (4-5 simple sentences is fine). You should:\n\nSummarize Narayanan’s key points in that section.\nIn one of the sentences, describe which aspects of the Uber case study (p. 13-16) reflect the ideas of the section you described.\n\nBring your paragraph in class and be ready to read it to your group."
  },
  {
    "objectID": "warmup-exercises.html#sec-vectorization",
    "href": "warmup-exercises.html#sec-vectorization",
    "title": "Warmup Exercises",
    "section": "Vectorization Brainstorm",
    "text": "Vectorization Brainstorm\nIn a recent lecture, we discussed methods of vectorizing text like the document-term matrix that use the bag of words assumption: the order of the words doesn’t matter!\nTake some time and propose an alternative approach to word-based text vectorization. Can you find a scheme that would give different vector representations to the following two sentences?\n\n“I love rabbits, not cats.” “I love cats, not rabbits.”\n\nYou don’t have to implement your vectorization, but please be prepared to write pseudocode for your group to show in detail how you would perform the vectorization."
  },
  {
    "objectID": "warmup-exercises.html#sec-compression",
    "href": "warmup-exercises.html#sec-compression",
    "title": "Warmup Exercises",
    "section": "Image Compression Factor of K-Means",
    "text": "Image Compression Factor of K-Means\nIn today’s reading on K-means clustering from the Python Data Science Handbook, Jake VanderPlas considers the use of K-means to reduce the number of distinct colors in an image (Example 2). I encourage you to run the code for this example while thinking about this warmup!\nGive an estimate of the compression factor: the reduction of information achieved when compressing an image using k-means clustering into \\(k\\) color clusters. The compression factor is the number of bits required to store the compressed image, divided by the number of bits required to store the original image. Both of these numbers can be computed asymptotically (i.e. with big-oh reasoning) in order to simplify the analysis.\nThere are multiple good ways to think about this question, and you’re welcome to choose one that makes sense to you as long as you carefully state your steps and assumptions. Here are a few points that I find helpful:\n\nBits in Original Image\n\nAn image with \\(n\\) rows and \\(m\\) columns has \\(nm\\) pixels.\nEach pixel has one of three RGB color channels (Red, Green, and Blue).\nEach color channel can be represented with 8 bits (which encode an integer between 0 and 255, denoting the color intensity in that channel).\n\n\n\nBits in Compressed Image\n\nIf I compress an image into just \\(k\\) distinct colors, then instead of storing the full RGB value for each pixel, I can just store enough bits to uniquely identify the cluster containing each pixel. How many bits do I need for this?\nI also need to store a dictionary (hash map) that associates color \\(j\\) (i.e. the centroid of the \\(j\\)th cluster of colors) to its RGB value.\n\n\n\nOptional Extra\nTry running the code above while varying the number of clusters. Do you think that a 16-color compression looks much better than an 8-color compression. Do you think the difference is good enough to justify approximately twice the storage? What about 32 colors vs. 16?"
  },
  {
    "objectID": "warmup-exercises.html#sec-intro-tensors",
    "href": "warmup-exercises.html#sec-intro-tensors",
    "title": "Warmup Exercises",
    "section": "Introducing Tensors",
    "text": "Introducing Tensors\nFirst, install PyTorch 2.0 into your ml-0451 Anaconda environment.\n\n\nThe best way to install PyTorch is is probably to run the following at the command line:\nconda activate ml-0451\npip3 install torch torchvision torchaudio\nThen, in a blank Jupyter notebook, copy, paste, and run each of the code blocks in the first section of the PyTorch tutorial.\nYou may need to do a little bit of exploring around the tutorials in order to come up with answers to these questions.\nFinally, write down a single-sentence answer to each of the following questions:\n\nIn what ways is a PyTorch tensor similar to a Numpy array?\nIn what ways is a PyTorch tensor different from a Numpy array?\nWhat is the primary motivation for the use of a specialized tensor data type, rather than an array, for deep learning?"
  },
  {
    "objectID": "warmup-exercises.html#sec-backprop",
    "href": "warmup-exercises.html#sec-backprop",
    "title": "Warmup Exercises",
    "section": "Efficient Differentiation",
    "text": "Efficient Differentiation\nThis exercise is based on a section of Chinmay Hegde’s notes on stochastic gradient descent and neural networks:\nConsider the following function: \\[\nL(w, b) = \\frac{1}{2} \\left(y - \\sigma(wx + b)\\right)^2 + \\frac{1}{2}\\lambda w^2\\;,\n\\]\nwhere \\(\\sigma(a) = \\frac{1}{1 + e^{-a}}\\).\nThis is the loss function that would be obtained when using a single feature \\(x\\) to predict \\(y\\), using the function \\(\\sigma(wx + b)\\) the predictor and measuring the quality of this predictor using the square-error loss function. Our aim is to compute the gradient of \\(L\\) with respect to \\(w\\) and \\(b\\), with a “ridge” regularization term \\(\\frac{1}{2}\\lambda w^2\\) that encourages the weight \\(w\\) to be small.\nI’ve used the property of the sigmoid that \\(\\sigma'(a) = \\sigma(a)(1-\\sigma(a))\\).\nThe gradient of \\(L\\) is \\(\\nabla L (w, b) = \\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b}\\right)\\), where\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial w} &= (\\sigma(wx + b) - y)\\sigma(wx+b)(1 - \\sigma(wx+b))x + \\lambda w \\\\\n\\frac{\\partial L}{\\partial b} &= (\\sigma(wx + b) - y)\\sigma(wx+b)(1 - \\sigma(wx+b))\\;.\n\\end{aligned}\n\\tag{3}\\]\n\nWhat You Should Do\nAssume that each of the following operations cost one computational unit:\n\nMultiplying or dividing two scalar numbers.\nAdding or subtracting two scalar numbers.\nComputing an exponential like \\(e^a\\).\n\nUsing this assumption:\n\nDetermine the number of computational units (i.e. computational cost) of computing the gradient of \\(L\\) exactly as written in Equation 3, under the assumption that you are not allowed to store the values of any intermediate computations.\nNow determine the computational cost of computing the gradient of \\(L\\) under the assumption that you are allowed to store intermediate computations. Please describe both the number of computations and the number of floating point numbers that must be stored.\nFinally, determine the computational cost in terms of both steps and storage to compute \\(L\\) using the backpropagation algorithm (described for a very similar function in Hegde’s notes).\n\nCompare your results from each method."
  },
  {
    "objectID": "warmup-exercises.html#sec-convolutional-kernel",
    "href": "warmup-exercises.html#sec-convolutional-kernel",
    "title": "Warmup Exercises",
    "section": "Convolutional Kernels",
    "text": "Convolutional Kernels\nImplement kernel convolution. Your implementation should accept a 2d array X (think of X as representing a greyscale image) and a square convolutional kernel K. Your implementation should operate using pure numpy. You can use any zero-padding strategy, but you do need to explain what your strategy is when presenting.\nIt’s ok to use a for-loop to loop over pixels.\nHere’s an example image you can use:\n\nfrom PIL import Image\nimport urllib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef read_image(url):\n    return np.array(Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\"\n\nimg = read_image(url)\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\nimg = to_greyscale(img)\n\nplt.imshow(img, cmap = \"Greys\")\nplt.gca().axis(\"off\")\n\n(-0.5, 639.5, 412.5, -0.5)\n\n\n\n\n\nAfter implementing your function, you should be able to use it like this, replacing the implementation of scipy.signal with your own implementation. The result should look something like this:\n\nfrom scipy.signal import convolve2d\n\nkernel = np.array([[-1, -1, -1], \n                   [-1,  8, -1], \n                   [-1, -1, -1]])\n\nconvd = convolve2d(img, kernel)\n\nplt.imshow(convd, cmap = \"Greys\", vmin = 0, vmax = 8)\nplt.gca().axis(\"off\")\n\n(-0.5, 641.5, 414.5, -0.5)"
  },
  {
    "objectID": "warmup-exercises.html#sec-project-check-in",
    "href": "warmup-exercises.html#sec-project-check-in",
    "title": "Warmup Exercises",
    "section": "Project Check-In",
    "text": "Project Check-In\nThis is a warmup activity that we will repeat weekly until the end of the semester.\nPrepare a 2-3 minute “presentation” of your project to your group. Your presentation can be informal and does not need to have any special visual aids. The primary expectation is that you are able to demonstrate some relevant functionality to your peers. If your project involves coding or data analysis, your relevant functionality might be as simple as accessing or preparing the data. You should plan to demonstrate additional functionality each week.\nIn other words, you should show your group something that works, regardless of how “big” it is.\nIf you are doing a project that does not involve implementation (such as a research essay), then you are still expected to offer an update. Your contributions could include describing the sources you’ve found or showing your group an outline of the argument that you will make.\nIt’s appropriate for each member of your project group to give the same presentation during warmup. Please note that you may not be in the same warmup group as your project partners. This means that:\n\nThe code you show needs to run on your laptop or in your compute instance (e.g. Google Colab).\nYou need to be ready to explain what is being shown, even if your project partners did much of the work."
  },
  {
    "objectID": "warmup-exercises.html#sec-transfer-learning",
    "href": "warmup-exercises.html#sec-transfer-learning",
    "title": "Warmup Exercises",
    "section": "What Needs To Be Learned?",
    "text": "What Needs To Be Learned?\nSuppose that you wanted to teach an individual to recognize English-language phishing emails. Write down a few features (based on the linked website, your own experience, or other sources) that you think would help someone classify an email as “phishing attempt or not” based on the text of the email.\nNow, imagine that you are going to sit down with your tutee to teach them how to recognize English-language phishing emails. Where would you start your instruction if…\n\nYour tutee was another member of your warmup group.\nYour tutee was a fluent English speaker but had never used email.\nYour tutee was a regular email user but spoke no English.\nYour tutee spoke no English and had never seen a computer.\n\nWhich of these four scenarios would require the most “learning effort?” Which would require the least?"
  },
  {
    "objectID": "warmup-exercises.html#sec-word-embedding",
    "href": "warmup-exercises.html#sec-word-embedding",
    "title": "Warmup Exercises",
    "section": "Word Embedding",
    "text": "Word Embedding\nTake out a sheet of paper and a pencil. Your goal is to place the following words on the sheet of paper in such a way that their location on the sheet is indicative of their relationships to each other. You can decide exactly how to do this. Should words with similar meanings be in the same part of the page? Should pairs of words with similar relationships have similar distances? Your approach is up to you, but please write it down along with your placements. Your words are:\n\nWoman\nStudent\nNurse\nDoctor\nMan\nProfessor\nModel\nComputer\nMachine\nProgrammer"
  },
  {
    "objectID": "warmup-exercises.html#sec-realistic-text",
    "href": "warmup-exercises.html#sec-realistic-text",
    "title": "Warmup Exercises",
    "section": "Realistic Text?",
    "text": "Realistic Text?\nIn our reading on The Unreasonable Effectiveness of Recurrent Neural Networks, there are a few examples of model output that is realistic but not real. Pick one of the examples, and write down as carefully as you are able what makes the generated text realistic. Then, describe what “tells” would tip off an attentive observer that the text isn’t real (generated intentionally by a human) after all.\nThe Shakespeare and Wikipedia examples might be the easiest ones to think about, but feel free to look at the \\(\\LaTeX\\) or Linux source code examples if you prefer."
  },
  {
    "objectID": "warmup-exercises.html#sec-mind-map",
    "href": "warmup-exercises.html#sec-mind-map",
    "title": "Warmup Exercises",
    "section": "Mind Map",
    "text": "Mind Map\nWow, we’ve covered a lot of ground in this class! Use a graphics program or a pen/paper to make a mind map describing some of our main theoretical and concepts. As a reminder, a mind-map is a graph in which the nodes are concepts and edges join related concepts. Please incorporate the following concepts as nodes:\n\nLoss function\nTarget\nPredictor\nModel\nRegression\nClassification\nEmpirical risk minimization\nGradient descent\nFeature map\nVectorization\nOverfitting\nTraining data\nValidation/testing data\nPerceptron\nLogistic regression\nNeural networks\nLinear regression\n\nAdditionally, please incorporate at least three other concepts of your own choosing.\n\nFlowcharts in Quarto\nSince mind-maps can be a little complicated to organize, you might find it easiest to work with some software. One optional possibility is actually included with Quarto: the Mermaid chart tool can render attractive diagrams that include labeled nodes and directed edges. Using a flowchart is probably the way. For example, inserting the following code into a special {mermaid} code block will produce the following diagram:\nflowchart TB\n    A(First concept) --&gt; B(Second concept)\n    B --is part&lt;br&gt; of--&gt; A\n    B--&gt;C(Third concept)\n    A--&gt;C\n    A & B & C --&gt;D(Fourth concept)\n\n\n\n\nflowchart TB\n    A(First concept) --&gt; B(Second concept)\n    B --is part&lt;br&gt; of--&gt; A\n    B--&gt;C(Third concept)\n    A--&gt;C\n    A & B & C --&gt;D(Fourth concept)\n\n\n\n\n\nFor the basics of using Mermaid with Quarto, see the Quarto docs. A benefit of this approach is that you don’t have to worry too much about positioning, and you can publish your mind map easily on your blog! On the other hand, this approach doesn’t give you much flexibility and makes it harder to be creative about incorporating complex relationships. Doing your mind map by hand or in any other software is entirely fine."
  },
  {
    "objectID": "warmup-exercises.html#sec-classification-rates",
    "href": "warmup-exercises.html#sec-classification-rates",
    "title": "Warmup Exercises",
    "section": "Classification Rates",
    "text": "Classification Rates\n\nPart 1\nCOVID-19 rapid tests have approximately an 80% sensitivity rate, which means that, in an individual who truly has COVID-19, the probability of a rapid test giving a positive result is roughly 80%.  On the other hand, the probability of a rapid test giving a positive result for an individual who truly does not have COVID-19 is 5%. Suppose that approximately 4% of the population are currently infected with COVID-19. These numbers are mostly made-up.Example 2.3.1 of Murphy, page 46, has a good review of the relevant probability and the definition of each of the rates below.\nWrite a Python function called rate_summary that prints the following output, filling in the correct values for each of the specified rates:\ns = 0.8           # test sensitivity\nf = 0.02          # probability of positive test if no COVID\nprevalence = 0.05 # fraction of population infected\n\nrate_summary(s, f, current_infection)\nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\n\nPart 2\n\nSuppose that scientists found an alternative rapid test which had a 75% sensitivity rate with a 0% chance of a positive test on someone who is truly not infected. Would you suggest replacing the old rapid tests with these alternative tests? Why? \nWhat if the alternative test had an 85% sensitivity rate and a 10% chance of a positive test on someone who is truly not infected?\n\nYou don’t necessarily need to use your function from the previous part in this part.\n\nPart 3\nIt’s all well and good to do the math, but what about when we actually have data? Write a function called rate_summary_2 that accepts two columns of a pandas.DataFrame (or equivalently two one-dimensional numpy.arrays of equal length). Call these y and y_pred. Assume that both y and y_pred are binary arrays (i.e. arrays of 0s and 1s). y represents the true outcome, whereas y_pred represents the prediction from an algorithm or test. Here’s an example of the kind of data we are thinking about:\n\nimport pandas as pd\n\nurl = \"https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/toy-classification-data.csv\"\ndf = pd.read_csv(url)\n\ndf.head() # just for visualizing the first few rows\n\n\n\n\n\n\n\n\ny\ny_pred\n\n\n\n\n0\n0\n0\n\n\n1\n1\n0\n\n\n2\n1\n0\n\n\n3\n0\n1\n\n\n4\n0\n0\n\n\n\n\n\n\n\nYou should be able to use your function like this:\n# y is the true label, y_pred is the prediction\nrate_summary_2(df[\"y\"], df[\"y_pred\"]) \nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\nHints\nAn excellent solution for this part will not use any for-loops. Computing each of the four rates can be performed in a single compact line of code. To begin thinking of how you might do this, you may want to experiment with code like the following:\ndf[[\"y\"]] == df[[\"y_pred\"]]\ndf[[\"y\"]].sum(), df[[\"y\"]].sum()"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is an advanced elective on the topic of algorithms that learn patterns from data. Artificial intelligence, predictive analytics, computational science, pattern recognition, signal processing, and data science are all disciplines that draw heavily on techniques from machine learning.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "syllabus.html#social-annotation",
    "href": "syllabus.html#social-annotation",
    "title": "Syllabus",
    "section": "Social Annotation",
    "text": "Social Annotation\nA an extremely useful way for you to engage with the readings is to make comments, ask questions, and answer questions about them as you are reading. For this reason, I’ll be providing most links to readings through Hypothes.is. You’ll be able to make marginal comments and view the marginal comments of others. I’ll also regularly be checking on your annotations to see what questions might have come up with the readings."
  },
  {
    "objectID": "syllabus.html#what-will-class-time-look-like",
    "href": "syllabus.html#what-will-class-time-look-like",
    "title": "Syllabus",
    "section": "What Will Class Time Look Like?",
    "text": "What Will Class Time Look Like?\nMy plan is for most class periods to look like a “lecture sandwich:”\n\n10 to 15 minutes of a warmup activity that addresses the recent lectures and readings.\n40-50 minutes of lecture, punctuated by short activities and breaks.\n10-15 minutes of a closing activity that helps us get solid with the day’s content.\n\n\nThe Warmup Activity\nOn most days, we’ll have a warmup activity. The warmup activity will usually ask you to engage with the readings and complete a small amount of work ahead of class time. This could be a short piece of writing, a math problem, or an implementation of a Python function.\nEach day, a few students will be randomly selected to present their work to a small group of peers. It’s ok to ask for help or even pass if you’re not feeling confident in your solution, but you should plan to at least make a good attempt at the warmup before every class period. Your participation on the warmup activity is an important aspect of presence in the course, and I’ll ask you to reflect on it when proposing your course grade."
  },
  {
    "objectID": "syllabus.html#collaborative-grading",
    "href": "syllabus.html#collaborative-grading",
    "title": "Syllabus",
    "section": "Collaborative Grading",
    "text": "Collaborative Grading\nThis course is collaboratively graded.  In a nutshell, this means:You may have also heard the term ungrading to refer to a similar approach.\n\nThere are no points or scores attached to any assignment. When you turn in assignments, you’ll get feedback on how to revise/resubmit, improve or otherwise proceed in the course, but you won’t get “graded.”\nThere also aren’t any firm due dates, although I will give you suggestions on how to maintain a good pace. \nPeriodically throughout the semester, you will complete reflection activities to help you take stock of your learning and achievement in the course. In your final activity at the end of the semester, you’ll make a proposal for your letter grade in the course, and support it with evidence of your learning. You and I will then meet to discuss how the course went for you, using your reflection activity and proposal as a starting point. In this conversation, you and I will agree on your final letter grade for the course, which I will then submit to the registrar.\n\nAll work you wish to be considered toward your achievement in the course needs to be submitted by the end of Finals Week.Reflection activities:\n\nReflective goal-setting.\nMid-course reflection.\nEnd-of-course reflection and grade proposal."
  },
  {
    "objectID": "syllabus.html#why-collaborative-grading",
    "href": "syllabus.html#why-collaborative-grading",
    "title": "Syllabus",
    "section": "Why Collaborative Grading?",
    "text": "Why Collaborative Grading?\nBecause grading is broken! Traditional points-based grading is ineffective at both (a) accurately measuring student learning and (b) motivating students to learn. I broadly agree with Jesse Stommel when he writes:\n\nAgency, dialogue, self-actualization, and social justice are not possible in a hierarchical system that pits teachers against students and encourages competition by ranking students against one another. Grades (and institutional rankings) are currency for a capitalist system that reduces teaching and learning to a mere transaction. Grading is a massive co-ordinated effort to take humans out of the educational process.\n\nI’d prefer to just not give you grades at all. But, Middlebury says I have to, and so my aim is to instead put the process of grading under your control to the greatest extent that I reasonably can."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nThere are three kinds of assessed assignments in this course, plus a mysterious “Other” category.\n\n\n\n\n\nBlog Posts\n\n\n\nBlog posts are the primary way in which you will demonstrate your understanding of course content. Blog posts usually involve: written explanation of some relevant theory; implementation one or more algorithms according to written specifications; performing experiments to test the performance of the implementations; and communicating findings in a professional way. Some blog posts will be more like short essays than problem sets or programming assignments. Your blog posts will be hosted on your own public website (which you will create). This website will serve as your portfolio for the course.\n\n\n\n\n\n\nProject\n\n\n\nYour project is a large-scale undertaking that you will design and complete, usually in a group of 2 or 3, over the course of the semester. Your project should usually involve some combination of data collection, implementation, research of related work, experimentation, deployment, or theory work (but not necessarily all components). Projects are expected to demonstrate deep engagement with both the course content and the problem selected.\n\n\n\n\n\nProcess Reflections\n\n\n\nAt the beginning of the course, you’ll write a process reflection describing your aspirations for the course—what you want to learn and achieve, and how you’d like to be assessed against your goals. We’ll have a second process reflection mid-way through the course that will allow you to reflect on your progress toward your objectives and consider changing direction if needed. At the end of the course, you’ll write a summary reflection on your learning, accomplishment, and engagement with the class. This is also the place where you’ll propose your final letter grade.   I’ll usually give you written feedback on your process reflections. We’ll also meet at the end of the course to discuss your final reflection and agree on your letter grade for the course.\n\n\n\n\n\nOther…?\n\n\n\nYou may have some topic or idea that especially interests you and which you want to explore. If you’d like to work on this topic and use it to demonstrate your learning in the course, you can propose it to me. I may have suggestions or requested modifications before I agree to count the work in your course portfolio."
  },
  {
    "objectID": "syllabus.html#best-by-dates",
    "href": "syllabus.html#best-by-dates",
    "title": "Syllabus",
    "section": "Best-By Dates",
    "text": "Best-By Dates\nWhile we don’t have formal due dates, there is a benefit to keeping yourself on a schedule. It’s best to complete assignments close to the time when we covered the corresponding content in class, and it’s important for your wellbeing not to let work pile up. I’ll provide “best-by” dates for all assignments. These are my recommendations for when you should submit the first versions of these assignments to me for feedback.\n\n\n Image credit: Dr. Spencer Bagley"
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Syllabus",
    "section": "Feedback",
    "text": "Feedback\nI won’t “grade” your individual assignments, but our course team and I will offer you feedback about what I thought was successful and where you can improve. My general expectation is that you will often (though not always) revise your work in response to feedback and resubmit it. Revising in response to feedback is one of the single most effective ways for you to deepen your learning.\nI’ll usually describe the importance of revisions on your assignment using one of the following categories:\n\nNo revisions suggested: you’ve done great work and should focus on the next thing.\nRevisions useful: you have opportunities for improvement on this assignment, but focusing on the next topic or assignment may be a better use of your time—use your judgment.\n\nRevisions encouraged: the best use of your time is to respond to feedback and resubmit, rather than moving on to the next assignment.\nIncomplete: the assignment isn’t sufficiently complete for it to be used as evidence of your learning."
  },
  {
    "objectID": "syllabus.html#what-work-do-you-need-to-do",
    "href": "syllabus.html#what-work-do-you-need-to-do",
    "title": "Syllabus",
    "section": "What Work Do You Need To Do?",
    "text": "What Work Do You Need To Do?\nAt the beginning of the semester, you’ll write a process letter that will outline what you’d like to learn and achieve in the course. It’s ok if you don’t meet all your aspirations by the end of the course. To help guide you in your goal-setting and work-planning, I do have some general expectations.\nI am likely to consider your time in my course to be highly successful if you do at least one of the following things:\nTime spent being stuck doesn’t count as “productive hours” – get help if you need it!\n\nYou complete almost all assignments with a high degree of quality, including revising in response to my feedback.\nYou spend on average 10 productive hours of work time on this course outside of class.\nYou complete many assignments that I give you, and also propose and complete alternative work that demonstrates your learning and achievement."
  },
  {
    "objectID": "syllabus.html#directing-your-learning",
    "href": "syllabus.html#directing-your-learning",
    "title": "Syllabus",
    "section": "Directing Your Learning",
    "text": "Directing Your Learning\nThis course asks you to set your own goals and motivate yourself to achieve them. Neither of these tasks are easy. It’s ok to mess up every now and then – we all do! The real question is whether you’re going to look at mistakes and make time to reflect on what to do next time."
  },
  {
    "objectID": "syllabus.html#programming",
    "href": "syllabus.html#programming",
    "title": "Syllabus",
    "section": "Programming",
    "text": "Programming\n\nYou can write moderately-complex, object-oriented software.\nYou are comfortable reading software documentation and researching how to perform a task that you haven’t seen before.\nYou know what a terminal is and how to perform simple operations at the command line.\nYou have experience debugging your code and you are ready to do it a lot more."
  },
  {
    "objectID": "syllabus.html#math",
    "href": "syllabus.html#math",
    "title": "Syllabus",
    "section": "Math",
    "text": "Math\nI am assuming that you remember most of MATH 0200 and CSCI 0200. It’s ok if you haven’t memorized every single fact. What I need is for you to be ready to rapidly look up what you need so that you won’t be slowed down by math along the way.\n\nMatrix multiplication and inner products\nEverything about \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\).\n\nVisualizing linear spaces.\nEigenvalues, eigenvectors, positive-definite matrices.\nDerivatives, critical points of functions.\nSample spaces, probability distribution functions.\nRandom variables, mean and variance.\nConditional probability and expectations.\n\n\nReviews/Diagnostics\n\nThis resource from Stanford’s CS246 contains most of the linear algebra that you’ll need for the course. The only big topic that’s missing is treatment of the existence of solutions of the linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) in terms of the rank of \\(\\mathbf{A}\\). You don’t need to have memorized everything here, but most of it should look familiar.\nProbability is not a formal requirement for CSCI 0451, but some probability can certainly be useful. To brush up on some basics, I suggest Chapter 2 of Introduction to Probability for Data Science by Stanley Chan. This treatment may be a little more advanced than what you learned in CSCI 0200, but you should recognize many of the main ideas."
  },
  {
    "objectID": "syllabus.html#covid-19-considerations",
    "href": "syllabus.html#covid-19-considerations",
    "title": "Syllabus",
    "section": "COVID-19 Considerations",
    "text": "COVID-19 Considerations\n\nMasks Are Required in CSCI 0451\nThe Computer Science Department policy states that:\n\nWe in the Computer Science department value a safe learning and working environment for all. While we can’t eliminate the risks associated with COVID-19, evidence suggests that widespread masking can significantly reduce the transmission and severity of disease. In order to protect the health of our community, the CS department recommends that students and faculty wear masks in CS learning spaces, including classrooms, office hours, and public areas. We acknowledge the College policy gives instructors the final say over classroom masking requirements, and expect all students to respect instructors’ stated policies in each course.\n\nIn alignment with this policy, I require you to wear masks in class and office hours. I encourage you to wear masks during help sessions and at all other times when you are inside 75 Shannon Street.\nIf you arrive in class without a mask, I will offer you one. I will expect you to either wear it or excuse yourself from class that day."
  },
  {
    "objectID": "syllabus.html#academic-integrity-and-collaboration",
    "href": "syllabus.html#academic-integrity-and-collaboration",
    "title": "Syllabus",
    "section": "Academic Integrity and Collaboration",
    "text": "Academic Integrity and Collaboration\n\nAcademic Integrity\nBriefly, academic integrity means that you assume responsibility for ensuring that the work you submit demonstrates your learning and understanding.\nTo be frank, it’s pretty easy to act without integrity (i.e. cheat) in this course. First, there’s a lot of solution code for machine learning tasks in Python online. Second, I’m literally asking you all to post your assignments publicly online. So, there are lots of opportunities to turn in assignments without actually doing the learning that those assignments are designed to offer you.\nI assume that both of us want you to learn some cool stuff. Cheating stops you from doing that, and ultimately wastes both your time and mine. I won’t be vigorously hunting for academic integrity violations, but I may ask you to discuss code or theory with me in class or in our meetings. If I notice you struggling to explain code that you submitted for feedback, I may have questions.\nTrust me. Neither of us want this."
  },
  {
    "objectID": "syllabus.html#collaboration",
    "href": "syllabus.html#collaboration",
    "title": "Syllabus",
    "section": "Collaboration",
    "text": "Collaboration\nI love it! Please collaborate in ways that allow you and your collaboration partners to fully learn from and engage with the content. Sharing small snippets of code or math is often helpful to get someone unstuck, but sharing complete function implementations or mathematical arguments is usually counterproductive.\nHere are some general guidelines for how I think about collaboration."
  },
  {
    "objectID": "syllabus.html#general-advice",
    "href": "syllabus.html#general-advice",
    "title": "Syllabus",
    "section": "General Advice",
    "text": "General Advice\nI am always happy to talk with you about your future plans, including internships, research opportunities, and graduate school applications. Because I am a creature of the academy, I am less knowledgeable about industry jobs, although you are welcome to ask about those too. You can drop in during Student Hours or email me to make an appointment."
  },
  {
    "objectID": "syllabus.html#letters-of-recommendation",
    "href": "syllabus.html#letters-of-recommendation",
    "title": "Syllabus",
    "section": "Letters of Recommendation",
    "text": "Letters of Recommendation\nWriting letters of recommendation for students is a fundamental part of my job and something that I am usually very happy to do. Here’s how to ask me for a letter."
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Asking for Help",
    "section": "",
    "text": "Asking for help is a fundamental part of how you will learn in CSCI 0451. Do it often. Here is some wisdom on this topic from the Best Cat On the Internet:\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "help.html#dont-get-stuck",
    "href": "help.html#dont-get-stuck",
    "title": "Asking for Help",
    "section": "Don’t Get Stuck",
    "text": "Don’t Get Stuck\nWe want to productively challenge you, which is different from letting you get stuck. If you’ve spent more than 30 minutes without making any progress or change in your understanding, then that’s likely a sign that you should consult a new reading or other resource, ask for help from a classmate, a Course Assistant, or me."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Course Project",
    "section": "",
    "text": "The course project for CSCI 0451 is an opportunity for you to demonstrate your learning against one or more of the course’s six learning objectives on a topic of your choosing. Here’s the big picture:\n\n\nThere are five deliverables associated with your project:\n\nA project proposal, due at the beginning of Week 7. The purpose of the proposal is for you and your group to carefully outline what you want to work on and explain why it’s feasible. Your proposal will be in the form of a README.md file in a shared GitHub repository that will house your software.\nA mid-project update due in Week 9 or 10. This will be a short, informal presentation in which you will share what you’ve done with the class.\nThe project software, (aka the GitHub repository itself) due at the end of finals week.\nA project report in the form of an extended blog post in which you explain what you did, relate it to existing work, and show your experiments or other findings. The report is due at the end of finals week.\nA project presentation during Week 12. The presentation will be 7-8 minutes and executed as a group. It should involve a visual aid, usually slides.\n\nI’ll share more detailed information on each of these deliverables later in the course.\n\n\n\nI expect that most students will complete their projects in teams of 2-3 students. Individual projects and groups of 4 students should seek my permission prior to submitting their project proposal and explain the reason for such a small (or large) group.\n\n\n\nRemember that we have six learning learning objectives in this course. The project is actually its own objective—that is, part of the course goal is for you to have the experience of initiating and pursuing an idea that you design. The other five objectives are:\n\nTheory\nImplementation\nNavigation\nExperimentation\nSocial Responsibility\n\nIn general, I expect most projects to address at least two of these learning objectives. For example, a project in which you implement and test a new algorithm would address Theory, Implementation, and Experimentation. A project in which you work with a data set that you care about on a learning task using existing tools could address Navigation and Experimentation. A project in which you replicated the findings of a recent study on algorithmic bias could address Experimentation and Social Responsibility. There are lots of valid possibilities. Your project proposal will address which of these learning objectives your project will address, and your final report will describe what you learned under each objective.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "project.html#project-description",
    "href": "project.html#project-description",
    "title": "Course Project",
    "section": "",
    "text": "The course project for CSCI 0451 is an opportunity for you to demonstrate your learning against one or more of the course’s six learning objectives on a topic of your choosing. Here’s the big picture:\n\n\nThere are five deliverables associated with your project:\n\nA project proposal, due at the beginning of Week 7. The purpose of the proposal is for you and your group to carefully outline what you want to work on and explain why it’s feasible. Your proposal will be in the form of a README.md file in a shared GitHub repository that will house your software.\nA mid-project update due in Week 9 or 10. This will be a short, informal presentation in which you will share what you’ve done with the class.\nThe project software, (aka the GitHub repository itself) due at the end of finals week.\nA project report in the form of an extended blog post in which you explain what you did, relate it to existing work, and show your experiments or other findings. The report is due at the end of finals week.\nA project presentation during Week 12. The presentation will be 7-8 minutes and executed as a group. It should involve a visual aid, usually slides.\n\nI’ll share more detailed information on each of these deliverables later in the course.\n\n\n\nI expect that most students will complete their projects in teams of 2-3 students. Individual projects and groups of 4 students should seek my permission prior to submitting their project proposal and explain the reason for such a small (or large) group.\n\n\n\nRemember that we have six learning learning objectives in this course. The project is actually its own objective—that is, part of the course goal is for you to have the experience of initiating and pursuing an idea that you design. The other five objectives are:\n\nTheory\nImplementation\nNavigation\nExperimentation\nSocial Responsibility\n\nIn general, I expect most projects to address at least two of these learning objectives. For example, a project in which you implement and test a new algorithm would address Theory, Implementation, and Experimentation. A project in which you work with a data set that you care about on a learning task using existing tools could address Navigation and Experimentation. A project in which you replicated the findings of a recent study on algorithmic bias could address Experimentation and Social Responsibility. There are lots of valid possibilities. Your project proposal will address which of these learning objectives your project will address, and your final report will describe what you learned under each objective."
  },
  {
    "objectID": "project.html#what-makes-a-good-project",
    "href": "project.html#what-makes-a-good-project",
    "title": "Course Project",
    "section": "What Makes a Good Project?",
    "text": "What Makes a Good Project?\n\nBig Picture\nThere’s a lot more detail on this topic below, but there are two simple questions that you should ask yourselves when envisioning your project:\n\nWill I learn something by completing this project?  Will I be proud of this project once it’s done?\n\nIf the answer to both questions is “yes,” then your overall project idea is likely good. Feel free to approach me early if you want to talk over whether your project idea is suitable for the course.\n\n\nBoring Projects\nThere is a kind of machine learning project that I feel is very boring and doesn’t really teach all that much. I’ll call these “Kaggle-style” projects. A “Kaggle-style” project is a project that starts with a convenient, clean data set and ends with a test score.  KS projects don’t clean or explore the data; don’t implement new algorithms; and don’t think carefully about why one algorithm might be better than another for the data in question. Instead, they simply try a bunch of things and assess them with a validation or test score.  I will probably not be impressed with a Kaggle-style project, partly because these kinds of projects really only address the “Navigation” learning objective.The website Kaggle is famous for hosting machine learning competitions in which the goal is to train a model that achieves the best prediction score on test data.I am being a little unfair; some Kaggle submissions are of exceptionally high quality.\nIt’s fine (indeed, encouraged!) for you to find some data that interests you and apply machine learning methods to it in order to make predictions or understand the structure of the data. To deepen your project beyond “Kaggle style,” you can incorporate some or all of the following:\n\nWork with data that is messy and needs significant processing before it can be used for ML tasks.\nDesign a custom vectorization scheme (i.e. way of representing each data points as a vector), or experiment with several pre-implemented schemes.\nConstruct multiple visualizations of your data that highlight patterns you wish to model or questions that you wish to explore.\nConduct a careful audit of your model to understand whether it performs better in some situations than others.\n\n\n\nCritical Discussion\nOne thing that should be incorporated into both your proposal and your project writeup is a critical discussion of incentives and impacts in your model.\nIncorporate a critical discussion of incentives and impacts in your work.\n\nIf someone were paying you to develop this model, who would be paying and why? Why might someone want this model to be built? Are you comfortable with that?\nWho are the users of your work? Who could be affected by your work? Are these populations the same?\nAre there risks of substantial bias or harm associated with the work you produce?"
  },
  {
    "objectID": "project.html#ideas",
    "href": "project.html#ideas",
    "title": "Course Project",
    "section": "Ideas",
    "text": "Ideas\nHere are some suggestions for choosing project directions. It’s fine for your project to be something entirely different—indeed, I encourage it! The primary benefit of projects that I come up with is that they are more likely to “work.” The primary drawback is that they may not be what you’re interested in! Even projects that don’t fully meet their stated objectives can still be successful experiences that demonstrate learning for the course.\n\nTheory and Implementation\nIf you enjoy thinking about math and how to translate math into performant numerical code, then you may wish to consider implementing a machine learning algorithm that we haven’t implemented in blog posts. There are many candidates, and it can be partially up to you to decide. Since the project is bigger than a blog post…\n\n…Your implementation should likely be of a more complex algorithm than ones we implemented in blog posts already. Alternatively, you could implement several related algorithms.\n…You will likely need to perform more complicated experiments in order to demonstrate the performance of your implementation.\n\nBecause these projects involve theoretical content that we haven’t covered in class, it is a good idea to talk with me before committing to one of them.\n\nSupport Vector Machine\nSupport vector machines (SVMs) were among the state-of-the-art binary classifiers before the rise of deep neural networks. Support vector machines are convex linear classifiers like logistic regression, but have some special mathematical properties that enable them to make much faster predictions on new data by leveraging sparsity. A good SVM project would likely involve most or all of the following:\n\nImplementing SVM using a version of stochastic gradient descent called PEGASOS (Fig. 2 in Shalev-Shwartz, Singer, and Srebro (2007)).\nImplementing kernel SVM, which enables nonlinear decision boundaries, using a quadratic solver.\nTesting your results on several real and synthetic data sets for:\n\nRuntime of training.\nRuntime of prediction.\nAccuracy of prediction.\n\n\n\n\nFaster Gradient Descent\nGradient descent and its relatives apply to a wide range of machine learning algorithms. Gradient descent comes in many flavors:\n\nStochastic\nMomentum\nAccelerated\nPrimal-dual methods\nModified gradient methods for nonconvex problems\nNewton methods\n\netc. etc. One good project could be to implement several of these methods for one or more ML algorithms, comparing the runtime of the training step on a variety of data sets.\n\n\nOther\nI can throw you lots of other theoretical/implementation problems if you’re interested—come chat and we can discuss."
  },
  {
    "objectID": "project.html#applied-analysis-projects",
    "href": "project.html#applied-analysis-projects",
    "title": "Course Project",
    "section": "Applied Analysis Projects",
    "text": "Applied Analysis Projects\nYou may wish to take machine learning methods that we discussed in class and apply them to data about some topic that you care about. Some great project interests that I’ve heard include remote sensing, sports analytics, genome analysis, and text analysis/language modeling."
  },
  {
    "objectID": "project.html#audits-and-algorithmic-bias",
    "href": "project.html#audits-and-algorithmic-bias",
    "title": "Course Project",
    "section": "Audits and Algorithmic Bias",
    "text": "Audits and Algorithmic Bias\nMany of you may have an interest in further exploring topics related to fairness and bias in machine learning algorithms. This is a great topic! Some possibilities include:\n\nThoroughly reproducing the findings of papers that diagnose algorithmic bias with an available data set or algorithm, such as Obermeyer et al. (2019).\nWrite a critical review essay on the topic of algorithmic bias in a specific area. This should be a polished essay with lots of references and some specific, quantitative examples."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Readings in normal font should be completed and annotated ahead of lecture.\nReadings in italic provide optional additional depth on the material.\n\nAssignments are listed on the day when I suggest you begin working on them.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "schedule.html#week-2",
    "href": "schedule.html#week-2",
    "title": "Schedule",
    "section": "Week 2",
    "text": "Week 2\n\n\n\n\n\n\n\n\n\n\n\nM\nFeb. 20\nConvex Linear Models and Logistic Regression\n\n\n\n\n\nWe discuss the modeling choices necessary to make the empirical risk minimization problem for linear classifiers tractable. In doing so we discuss convex functions and some of their properties that are relevant for optimization. Finally, we introduce logistic regression as an example of a convex linear classifier.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nDaumé 2.1-2.7\nDaumé 7.1-7.3\nHardt and Recht, p. 70-77\n\nNotes\nLecture notes\n\nWarmup\nConvexity\n\n\n\nW\nFeb. 22\nOptimization via Gradient Descent\n\n\n\n\n\nWe discuss standard mathematical methods for empirical risk minimization, including gradient descent and stochastic gradient descent. We also recontextualize the perceptron algorithm as stochastic subgradient descent for a linear classifier with a specific loss function.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nDaumé 7.4-7.6\nDiesenroth, Faisal, and Soon, p. 225-233\n\nNotes\nLecture notes\n\nWarmup\nGradient Descent\nAssignments\nBlog post: gradient descent"
  },
  {
    "objectID": "schedule.html#week-3",
    "href": "schedule.html#week-3",
    "title": "Schedule",
    "section": "Week 3",
    "text": "Week 3\n\n\n\n\n\n\n\n\n\n\n\nM\nFeb. 27\nFeatures, Regularization, and Nonlinear Decision Boundaries\n\n\n\n\n\nWe learn how to use feature maps to help our convex linear classifiers learn nonlinear patterns. We also introduce the problem of overfitting and introduce feature selection and regularization as methods for addressing this problem.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\nNavigation\nExperimentation\n\nReading\nIntroducing Scikit-Learn\nHyperparameters and Model Validation\nFeature Engineering\n\nNotes\nLecture notes\nLive version\n\nWarmup\nGradient Descent Again\nAssignments\nACTUAL REAL DUE DATE: Reflective Goal-Setting due 2/27\n\n\nW\nMar. 01\nClassification in Practice\n\n\n\n\n\nWe work through a complete modeling workflow for the Titanic survival data set. Along the way, we work with data frames and discuss cross-validation.\n\n\n\n\nLearning Objectives\nNavigation\nExperimentation\n\nReading\nDaumé Chapter 2 You may find it useful to review Chapter 1 as well.\nData Manipulation with Pandas (Focus on the sections up to and including \"Aggregation and Grouping\")\n\nNotes\nLecture notes\nLive version\n\nWarmup\nOverfitting and the Scientific Method\nAssignments\nBlog post: kernel logistic regression\nOR\nBlog post: penguins"
  },
  {
    "objectID": "schedule.html#week-4",
    "href": "schedule.html#week-4",
    "title": "Schedule",
    "section": "Week 4",
    "text": "Week 4\n\n\n\n\n\n\n\n\n\n\n\nM\nMar. 06\nBeyond Convex Linear Classifiers\n\n\n\n\n\nWe discuss several examples of other classifiers at a high level, including some that are nonlinear or nonconvex.\n\n\n\n\nLearning Objectives\nNavigation\n\nReading\nNA\n\nNotes\nLecture notes\nLive version\n\n\n\n\n\nW\nMar. 08\nLinear Regression\n\n\n\n\n\nWe introduce linear regression, another convex linear model suitable for predicting real numbers instead of class labels.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nNA\n\nNotes\nLecture notes\nLive version\n\n\nAssignments\nBlog post: Linear regression"
  },
  {
    "objectID": "schedule.html#week-5",
    "href": "schedule.html#week-5",
    "title": "Schedule",
    "section": "Week 5",
    "text": "Week 5\n\n\n\n\n\n\n\n\n\n\n\nM\nMar. 13\nIntroduction to Bias and Fairness\n\n\n\n\n\nTBD\n\n\n\n\nLearning Objectives\nSocial Responsibility\nExperimentation\n\nReading\nMachine Bias by Julia Angwin et al. for ProPublica.\nFair prediction with disparate impact by Alexandra Chouldechova, Sections 1 and 2.\nInherent trade-offs in the fair determination of risk scores by Jon Kleinberg et al, pages 1-5.\n\nNotes\nLecture notes\nLive version\n\nWarmup\nBalancing Classification Rates\n\n\n\nW\nMar. 15\nCritical Perspectives\n\n\n\n\n\nWe discuss limitations of the quantitative approach to studying discrimination, as well as critical perspectives on the role that automated decision systems play in surveilling and controlling marginalized individuals.\n\n\n\n\nLearning Objectives\nSocial Responsibility\nExperimentation\n\nReading\nThe Limits of the Quantitative Approach to Discrimination, speech by Arvind Narayanan\n\"The Digital Poorhouse\" by Virginia Eubanks for Harper's Magazine\n\nNotes\nTBD\n\nWarmup\nLimits of the Quantitative Approach\nAssignments\nBlog post: Limits of quantitative methods\nOR\nBlog post: Auditing allocative bias"
  },
  {
    "objectID": "schedule.html#week-6",
    "href": "schedule.html#week-6",
    "title": "Schedule",
    "section": "Week 6",
    "text": "Week 6\n\n\n\n\n\n\n\n\n\n\n\nM\nMar. 27\nVectorization\n\n\n\n\n\nWe discuss some ways by which complex objects like images and especially text can be represented as numerical vectors for machine learning algorithms.\n\n\n\n\nLearning Objectives\nNavigation\nExperimentation\n\nReading\nMurphy, Chapter 1. This is not related to vectorization; it's for you to get oriented on some possible project ideas. Don't worry about any math you don't understand.\nCourse project description\n\nNotes\nLecture notes\nLive version\n\nWarmup\nPitch a Project Idea\nAssignments\nACTUAL REAL DUE DATE: Mid-semester reflection due 4/05\n\n\nW\nMar. 29\nIntroducing Unsupervised Learning: Topic Modeling\n\n\n\n\n\nWe begin to discuss unsupervised learning, with topic modeling as our initial example.\n\n\n\n\nLearning Objectives\nTheory\nNavigation\nExperimentation\n\nReading\nPrincipal Component Analysis from the Python Data Science Handbook\n\nNotes\nLecture notes\nLive version\n\nWarmup\nVectorization Brainstorm\nAssignments\nACTUAL REAL DUE DATE: Project Proposal due 4/07"
  },
  {
    "objectID": "schedule.html#week-7",
    "href": "schedule.html#week-7",
    "title": "Schedule",
    "section": "Week 7",
    "text": "Week 7\n\n\n\n\n\n\n\n\n\n\n\nM\nApr. 03\nClustering Data\n\n\n\n\n\nWe continue our discussion of unsupervised learning with two methods for clustering sets of data.\n\n\n\n\nLearning Objectives\nTheory\nNavigation\nExperimentation\n\nReading\nK-Means Clustering from the Python Data Science Handbook\n\nNotes\nLecture notes\nLive version\n\nWarmup\nK-Means Compression\nAssignments\nBlog post: Unsupervised learning with linear algebra (however, using this time to complete a previous blog post is also highly recommended)\n\n\nW\nApr. 05\nIntroducing Deep Learning\n\n\n\n\n\nWe begin our discussion of deep learning with a quick theoretical motivation and a first glance at the PyTorch package.\n\n\n\n\nLearning Objectives\nTheory\nNavigation\n\nReading\nLecture 1, Introduction from Chinmay Hegde's course on deep learning at NYU\n\nNotes\nLecture notes\nLive version\n\nWarmup\nIntroducing Tensors"
  },
  {
    "objectID": "schedule.html#week-8",
    "href": "schedule.html#week-8",
    "title": "Schedule",
    "section": "Week 8",
    "text": "Week 8\n\n\n\n\n\n\n\n\n\n\n\nM\nApr. 10\nOptimization For Deep Learning\n\n\n\n\n\nWe begin a discussion of the training process for neural networks, which requires efficient computation of gradients via backpropagation and efficient variations of gradient descent.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nLecture 2, Neural Nets from Chinmay Hegde's course on deep learning at NYU\n\nNotes\nLecture notes\nLive version\n\nWarmup\nEfficient Differentiation\nAssignments\nBlog post: Optimization with Adam (however, using this time to complete a previous blog post is also highly recommended)\n\n\nW\nApr. 12\nConvolutional Neural Networks\n\n\n\n\n\nWe discuss methods for image classification using neural networks and introduce convolutional layers.\n\n\n\n\nLearning Objectives\nTheory\nExperimentation\n\nReading\nConvolutional Neural Networks from MIT's course 6.036.\nA Comprehensive Guide to Convolutional Neural Networks by Sumit Saha on Towards Data Science has some good visuals.\n\nNotes\nLecture notes\nLive version\n\nWarmup\nConvolutional Kernels\nAssignments\nACTUAL REAL DUE DATE: Engaging with Timnit Gebru\nPart 1 due 4/19"
  },
  {
    "objectID": "schedule.html#week-9",
    "href": "schedule.html#week-9",
    "title": "Schedule",
    "section": "Week 9",
    "text": "Week 9\n\n\n\n\n\n\n\n\n\n\n\nM\nApr. 17\nMore on Image Classification\n\n\n\n\n\nWe continue our discussion of image classification with convolutional neural networks.\n\n\n\n\nLearning Objectives\nExperimentation\nNavigation\n\nReading\nConvolutional Neural Networks from MIT's course 6.036.\nA Comprehensive Guide to Convolutional Neural Networks by Sumit Saha on Towards Data Science has some good visuals.\n\nNotes\nLecture notes\nLive version\n\nWarmup\nProject Check-In\n\n\n\nW\nApr. 19\nSome Practical Techniques in Image Classification\n\n\n\n\n\nWe discuss data augmentation and transfer learning, two helpful techniques in image classification. We also highlight some of messy challenges involving management of complex data for classification tasks with PyTorch.\n\n\n\n\nLearning Objectives\nTheory\nExperimentation\n\n\nNotes\nLecture notes\nLive version\n\nWarmup\nHow Much Needs To Be Learned?"
  },
  {
    "objectID": "schedule.html#week-10",
    "href": "schedule.html#week-10",
    "title": "Schedule",
    "section": "Week 10",
    "text": "Week 10\n\n\n\n\n\n\n\n\n\n\n\nM\nApr. 24\nDr. Timnit Gebru on Computer Vision and \"Artificial General Intelligence\"\n\n\n\n\n\nWe speak with Dr. Timnit Gebru about her recent work on computer vision and ideology in artificial general intelligence.\n\n\n\n\nLearning Objectives\nSocial Responsibility\n\n\n\nWarmup\nProject Check-In\n\n\n\nW\nApr. 26\nText Classification and Word Embeddings\n\n\n\n\n\nWe begin our study of text classification and the use of word embeddings for efficient text vectorization.\n\n\n\n\nLearning Objectives\nTheory\nExperimentation\n\nReading\nEfficient Estimation of Word Representations in Vector Space by Mikolov et al. (sections 1, 4, 5)\n\nNotes\nLecture notes\nLive version\n\nWarmup\nWord embedding\nAssignments\nBlog post: deep music classification"
  },
  {
    "objectID": "schedule.html#week-11",
    "href": "schedule.html#week-11",
    "title": "Schedule",
    "section": "Week 11",
    "text": "Week 11\n\n\n\n\n\n\n\n\n\n\n\nM\nMay. 01\nWord Embeddings\n\n\n\n\n\nWe continue our study of text classification by training a classifier and examining word embeddings.\n\n\n\n\nLearning Objectives\nTheory\nExperimentation\nSocial Responsibility\n\nReading\nMan is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. Can you explain how orthogonal projections can help reduce bias in word embeddings?\nFair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor\n\nNotes\nLecture notes\nLive version\n\nWarmup\nProject Check-In\n\n\n\nW\nMay. 03\nText Generation and Recurrent Neural Networks\n\n\n\n\n\nWe use recurrent neural networks to generate synthetic text with several realistic attributes.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\nNavigation\n\nReading\nThe Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy\n\nNotes\nLecture notes\nLive version\n\nWarmup\n\"Realistic\" text"
  },
  {
    "objectID": "schedule.html#week-12",
    "href": "schedule.html#week-12",
    "title": "Schedule",
    "section": "Week 12",
    "text": "Week 12\n\n\n\n\n\n\n\n\n\n\n\nM\nMay. 08\nReflection and Feedback\n\n\n\n\n\nWe look back on our time in the course, reflect on the responsibilities of data scientists in society, and give feedback on the course.\n\n\n\n\nLearning Objectives\nTheory\nSocial Responsibility\n\nReading\nMillions of black people affected by racial bias in health-care algorithms by Heidi Ledford for Nature\n(Optional) Dissecting racial bias in an algorithm used to manage the health of populations by Obermeyer et al. in Science.\n\n\nWarmup\nConcept Mind Map\n\n\n\nW\nMay. 10\nFinal Project Presentations\n\n\n\n\n\nWe present our final projects in CSCI 0451!\n\n\n\n\nLearning Objectives\nProject\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#finals-period",
    "href": "schedule.html#finals-period",
    "title": "Schedule",
    "section": "Finals Period",
    "text": "Finals Period\nDuring the reading and final exam period, you’ll meet with me 1-1 for about 15 minutes. The purpose of this meeting is to help us both reflect on your time in the course and agree on a final grade."
  },
  {
    "objectID": "schedule.html#due-dates",
    "href": "schedule.html#due-dates",
    "title": "Schedule",
    "section": "Due Dates",
    "text": "Due Dates\nIt’s best to submit all work that you wish to demonstrate your learning by the time of our final meeting. However, I will accept and assess work submitted by the last day of the final exam period."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "CSCI 0451: Machine Learning is an advanced elective at Middlebury College on the topic of algorithms that learn patterns from data. Artificial intelligence, predictive analytics, computational science, pattern recognition, signal processing, and data science are all disciplines that draw heavily on techniques from machine learning.\nYou can access this page using go/cs-451.\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Index of Assignments",
    "section": "",
    "text": "Process Reflections\n\n\n\n\n\nNo matching items\n\n\n\n\nBlog Posts\n\n\n\n\n\nNo matching items\n\n\n\n\nProject\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n  © Phil Chodrow, 2023"
  }
]